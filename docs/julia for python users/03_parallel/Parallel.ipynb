{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# A (very) Brief Look at Parallelization\n",
    "\n",
    "([Back to Overview](../index.html#/0/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here we will look at how parallel computing can be achieved from within Julia. The nice thing about Julia is that parallel computing is baked into the standard library. This is only a very brief overview (and I will elaborate more on this in the future), mainly targeted at running MD simulations. \n",
    "\n",
    "For more details on fine-grained control like `@spawn`, take a look at: https://docs.julialang.org/en/stable/manual/parallel-computing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Conotroling the number of workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "You can control the number of worker processes from the command line by running `julia -p n` where $n$ is the number of workers. Alternatively, we can use the `addprocs` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Distributed\n",
    "using SharedArrays\n",
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Int64}:\n",
       " 1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addprocs(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which spawned 8 worker threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prarallel `for` loops and `@parallel` reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The easiest way to perform an operation in parallel is to use the `@parallel` macro. More details can be found here: https://docs.julialang.org/en/stable/manual/parallel-computing/#Parallel-Map-and-Loops-1\n",
    "\n",
    "In the example below, we throw a \"coin\" 200000000 times, and count how many heads there are by casting the output of the `rand` command. So\n",
    "\n",
    "```julia\n",
    "Int(rand(Bool))\n",
    "```\n",
    "\n",
    "would give $0$ with  probability of $0.5$, and $1$ the rest of time. Since want to do this in parallel using the `@parallel` macro together with a `for` loop. Note that `@parallel for i = 1:200000000` would run through the loop in parallel, but would not send the result back to the controler. That's where the `(+)` function comes in. It applies the `+(.,.)` function to the result of each loop iteration and \"reduces\" the result of all these parallel runs into a single variable (`nheads`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_heads_parallel (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function n_heads()\n",
    "    n = 0\n",
    "    for i = 1:200000000\n",
    "        n += Int(rand(Bool))\n",
    "    end\n",
    "    n\n",
    "end\n",
    "\n",
    "function n_heads_parallel()\n",
    "    nheads = @distributed (+) for i = 1:200000000\n",
    "        Int(rand(Bool))\n",
    "    end\n",
    "    nheads\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     529.141 ms (0.00% GC)\n",
       "  median time:      544.115 ms (0.00% GC)\n",
       "  mean time:        547.785 ms (0.00% GC)\n",
       "  maximum time:     579.280 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark(n_heads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  27.59 KiB\n",
       "  allocs estimate:  682\n",
       "  --------------\n",
       "  minimum time:     160.312 ms (0.00% GC)\n",
       "  median time:      181.090 ms (0.00% GC)\n",
       "  mean time:        185.900 ms (0.00% GC)\n",
       "  maximum time:     263.847 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          27\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark(n_heads_parallel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's do something more reminiscent of MD simulations: let's fill an array (`a`) some sort of computed index. It might be very tempting to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x0000000107f65210"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = zeros(10)\n",
    "@distributed for i = 1:10\n",
    "    a[i] = i^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Which seems to not have done anything?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "This might seem odd, since the for loop definitely did _work_. But if you look at the output from the `@parallel for` loop you might notice all of these `Future` data types. These are future calls (from the perspective of thread 1, i.e. the control thread): they might or might not have happened. And they _definitely_ will take place in the \"future\" from the time you hit <shift+enter>. So basically, the work in the loop was done (in the future), and you just haven't \"collected\" the data onto the control thread's copy of the array `a`.\n",
    "\n",
    "Let's test our hypothesis by spawing a remote call (a `Future`) on a different thread to retrieve the global `a` stored there, and returning a remote reference `Bref`. We can then fetch the output back to the control thread using the `fetch(Bref)` command.\n",
    "\n",
    "https://docs.julialang.org/en/stable/manual/parallel-computing/#Data-Movement-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 1.0\n",
       " 4.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bref = @spawnat 2 a\n",
    "fetch(Bref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       "  0.0\n",
       "  0.0\n",
       "  9.0\n",
       " 16.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bref = @spawnat 3 a\n",
    "fetch(Bref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Aha! So we're doing work... it's just that we need to synchronize the data in the array `a`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Use `SharedArray` for Data Movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Not to be confused with [Distributed Arrays](https://github.com/JuliaParallel/DistributedArrays.jl) (`DArray`), `SharedArray`s make all their data available accross all threads. Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x0000000106cf4250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SharedArray{Float64}(10)\n",
    "@distributed for i = 1:10\n",
    "    a[i] = i^2\n",
    "    a[i] += 1 # implicit  comms!\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element SharedArray{Float64,1}:\n",
       "   1.0\n",
       "   4.0\n",
       "   9.0\n",
       "  16.0\n",
       "  25.0\n",
       "  36.0\n",
       "  49.0\n",
       "  64.0\n",
       "  81.0\n",
       " 100.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "... magic! There is a price to pay for this kind of convenience, but we'll see that later. So if you want a more fine-grained control over the communications and memory foot print, Distributed Arrays might be a better (if not more tedious) choice.\n",
    "\n",
    "https://github.com/JuliaParallel/DistributedArrays.jl#distributed-arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The `@everywhere` Macro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Before we continue with `SharedArray`, let's make a brief tangent to the oh-so-useful `@everywhere` macro. As the name suggests, it runs a command... well... everywhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@everywhere id = myid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "      From worker 8:\t8\n",
      "      From worker 2:\t2\n",
      "      From worker 4:\t4\n",
      "      From worker 5:\t5\n",
      "      From worker 3:\t3\n",
      "      From worker 7:\t7\n",
      "      From worker 9:\t9\n",
      "      From worker 6:\t6\n"
     ]
    }
   ],
   "source": [
    "@everywhere println(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "... which is very useful for making functions/modules available to all the workers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
